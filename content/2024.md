---
title: "MRAC: Multimodal, Generative and Responsible Affective Computing (ACM-MM 2024)" 
date: 2024-04-04T00:06:56+08:00
---

<br>
<div class="row">
  <div>
    <p><center>
        <img class="img-fluid banner-pic" src="/2024/MRAC_2024_banner.PNG">
    </center></p>
    <p><center>
      Half-day, <font size="3" color="red"> Oct 2024, Melbourne</font> 
    </center></p>
  </div>
</div><br>


## Introduction

Affective Computing involves the creation, evaluation and deployment of Emotion AI and Affective technologies to make people’s lives better. The creation, evaluation and deployment stages of the emotion-ai model require large amounts of multimodal data from RGB images to video, audio, text, and physiological signals. In principle, the development of any AI system must be guided by a concern for its human impact. The aim should be striving to augment and enhance humans, not replace humans; while taking inspiration from human intelligence, safely. To this end, the MRAC 2024 workshop aims to transfer the same concepts from a small-scale, lab-based environment to a real-world, large-scale corpus enhanced with responsibility. The workshop also aims to bring to the attention of researchers and industry professionals of the potential implications of generative technology along with its ethical consequences. 


## Call for Contributions

### Full Workshop Papers

The 2nd International Workshop on Multimodal, Generative and Responsible Affective Computing (MRAC 2024) at <a href="https://2024.acmmm.org/" target="_blank">ACM-MM 2024</a> (track for Multimodal and Responsible Affective Computing) aims to encourage and highlight novel strategies for affective phenomena estimation and prediction with a focus on robustness and accuracy in extended parameter spaces, spatially, temporally, spatio-temporally and most importantly Responsibly. This is expected to be achieved by applying novel neural network architectures, generative ai, incorporating anatomical insights and constraints, introducing new and challenging datasets, and exploiting multi-modal training. Specifically, the workshop topics include (but are not limited to):

- Large scale data generation or Inexpensive annotation for Affective Computing
- Generative AI for Affective Computing using multimodal signals
- Multi-modal method for emotion recognition
- Privacy preserving large scale emotion recognition in the wild
- Generative aspects of affect analysis
- Deepfake generation, detection and temporal deepfake localization
- Multimodal data analysis
- Affective Computing Applications in education, entertainment & healthcare
- Explainable or Privacy Preserving AI in affective computing
- Generative and responsible personalization of affective phenomena estimators with few-shot learning
- Bias in affective computing data (e.g. lack of multi-cultural datasets)
- Semi-/weak-/un-/self- supervised learning methods, domain adaptation methods, and other novel methods for Affective Computing 

<br>

We will be accepting the submission of full unpublished and original papers. These papers will be peer-reviewed via a double-blind process, and will be published in the official workshop proceedings and be presented at the workshop itself. 

#### Submission 
We invite authors to submit unpublished papers (6 to 8 page <a href="https://2024.acmmm.org/regular-papers" target="_blank">ACM-MM format</a>) to our workshop, to be presented at an oral/poster session upon acceptance. All submissions will go through a double-blind review process. All contributions must be submitted (along with supplementary materials, if any) at the <a href = " https://openreview.net/group?id=acmmm.org/ACMMM/2024/Workshop/MRAC" target="_blank">OpenReview</a>. Accepted papers will be published in the official ACM-MM Workshops proceedings.

#### Note
Authors of previously rejected main conference submissions are also welcome to submit their work to our workshop. When doing so, you must submit the previous reviewers' comments (named as previous\_reviews.pdf) and a letter of changes (named as letter\_of\_changes.pdf) as part of your supplementary materials to clearly demonstrate the changes made to address the comments made by previous reviewers.
## Important Dates


<table class="table table-striped">
    <tbody>
        <tr>
          <td>Paper Submission Deadline</td>
          <td>July 19, 2024 (12:00 Pacific time)</td>
        </tr>
        <tr>
          <td>Notification to Authors</td>
          <td>Aug 4, 2024</td>
        </tr>
        <tr>
          <td>Camera-Ready Deadline</td>
          <td>Aug 11, 2024 (12:00 Pacific time)</td>
        </tr>
    </tbody>
</table>

## Workshop Schedule
TBD

## Invited Keynote Speakers
<div class="row">
    <div class="col-3">
        <a href="https://research.unsw.edu.au/people/professor-julien-epps">
            <img class="speaker-pic pull-right" src="/2024/je.jpeg" />
        </a>
    </div>
    <div class="col">
        <a href="https://research.unsw.edu.au/people/professor-julien-epps"><span class="fs-3">Prof. Julien Epps<span class="fs-3"></span></a>
        <h6 class="fs-5">University of New South Wales</h6>
      <Strong>Biography:</Strong> Prof. Julien Epps is Professor in Digital Signal Processing and Dean of Engineering at The University of New South Wales, Sydney, Australia, where he was previously Head of School of the School of Electrical Engineering and Telecommunications. He also have an appointment as a Co-Director of the NSW Smart Sensing Network. He is a Scientific Advisor for Boston-based startup Sonde Health, where he have worked on speech-based assessment of mental health, and have held an appointment as a Contributed Principal Researcher with Data61, CSIRO, where he worked on methods for automatic task analysis using behavioural and physiological signals. A passionate educator, he is an Emeritus Fellow of the UNSW Scientia Education Academy. His research interests also include applications of speech modelling and processing, in particular to emotion and mental state recognition from speech and signals from wearable sensors. He has also worked on genomic sequence processing and aspects of human-computer interaction, including multimodal interfaces and computer-supported cooperative work.
    </div>
</div>
<br>
<br>
<div class="row">
    <div class="col-3">
        <a href="https://research-repository.uwa.edu.au/en/persons/mohammed-bennamoun">
            <img class="speaker-pic pull-right" src="/2024/mb.jpeg" />
        </a>
    </div>
    <div class="col">
        <a href="https://research-repository.uwa.edu.au/en/persons/mohammed-bennamoun"><span class="fs-3">Prof. Mohammed Bennamoun</span></a>
        <h6 class="fs-5">University of Western Australia</h6>
        <Strong>Biography:</Strong> Prof. Mohammed Bennamoun is currently a Winthrop Professor at the University of Western Australia. He served as the Head of the School of Computer Science and Software Engineering at UWA for five years (February 2007-March 2012). He was an Erasmus Mundus Scholar and Visiting Professor in 2006 at the University of Edinburgh. He was also Visiting Professor at CNRS (Centre National de la Recherche Scientifique) and Telecom Lille1, France in 2009, the Helsinki University of Technology in 2006, and the University of Bourgogne and Paris 13 in France in 2002-2003. He won the UWA Vice-Chancellor’s Research Mentorship Award in 2016. He also won the UWA Award for Teaching Excellence for Research Supervision in 2016. He was congratulated for his “outstanding contributions and for going above and beyond, to inspire, support and educate”. He won the “Best Supervisor of the Year” Award at QUT. He also received an award for research supervision at UWA in 2008. His areas of interest include computer vision (particularly 3D) e.g., object recognition & biometrics; machine/deep learning; robotics (e.g., obstacle avoidance and robot grasping); signal/image processing; control theory. 
    </div>
</div>

<br>

<style>
    .speaker-pic {
        width: 250px;
        height: 250px;
    }
</style>

## Organizers

<div class="container p-0">
  <div class="row">
    <div class="col">
        <a href="https://staffportal.curtin.edu.au/staff/profile/view/shreya-ghosh-a2f9d3ca/">
            <img class="organizer-pic" src="/2023/img/people/ShreyaGhosh.jpg"/> 
        </a>
        <div class="people-name orgnizer-people-name">
            <a href="https://staffportal.curtin.edu.au/staff/profile/view/shreya-ghosh-a2f9d3ca/">Shreya Ghosh</a>
            <h6 class="uni-name">Curtin University</h6>
        </div>
    </div>
    <div class="col">
        <a href="https://research.monash.edu/en/persons/zhixi-cai">
            <img class="organizer-pic" src="/2023/img/people/zc.jpeg">
        </a>
        <div class="people-name orgnizer-people-name">
            <a href="https://research.monash.edu/en/persons/zhixi-cai">Zhixi Cai</a>
            <h6 class="uni-name">Monash University</h6>
        </div>
    </div>
    <div class="col">
        <a href="https://www.flinders.edu.au/people/abhinav.dhall">
            <img class="organizer-pic" src="/2023/img/people/AbhinavDhall.jpg">
        </a>
        <div class="people-name orgnizer-people-name">
            <a href="https://www.flinders.edu.au/people/abhinav.dhall">Abhinav Dhall</a>
            <h6 class="uni-name">Flinders University</h6>
        </div>
    </div>
    <div class="col">
        <a href="http://eecs.qmul.ac.uk/people/profiles/kolliasdimitrios.html">
            <img class="organizer-pic" src="/2023/img/people/DimitriosKollias.jpg">
        </a>
        <div class="people-name orgnizer-people-name">
            <a href="http://eecs.qmul.ac.uk/people/profiles/kolliasdimitrios.html">Dimitrios Kollias</a>
            <h6 class="uni-name">Queen Mary University of London</h6>
        </div>
    </div>
    <div class="col">
        <a href="https://researchprofiles.canberra.edu.au/en/persons/roland-goecke">
            <img class="organizer-pic" src="/2023/img/people/RolandGoetcke.jpg">
        </a>
        <div class="people-name orgnizer-people-name">
            <a href="https://researchprofiles.canberra.edu.au/en/persons/roland-goecke">Roland Goecke</a>
            <h6 class="uni-name">UNSW Canberra</h6>
        </div>
    </div>
    <div class="col">
        <a href="https://staffportal.curtin.edu.au/staff/profile/view/tom-gedeon-5e48a1fd/">
            <img class="organizer-pic" src="/2023/img/people/TomGedeon.jpg">
        </a>
        <div class="people-name orgnizer-people-name">
            <a href="https://staffportal.curtin.edu.au/staff/profile/view/tom-gedeon-5e48a1fd/">Tom Gedeon</a>
            <h6 class="uni-name">Curtin University</h6>
        </div>
    </div>
  </div>
</div>

<style>
.organizer-pic {
    width: 150px;
    height: 150px;
}
.uni-name {
    max-width: 150px
}

.people-name {
    max-width: 150px;
} 

.orgnizer-people-name {
    text-align: center;
}

.speaker-pic, .organizer-pic {
    border-radius: 50%;
}

.banner-pic {
    width: 900px;
    height: auto;
}
</style>

## Program Committee (To be updated)

<div class="container p-0">
    <div class="row row-cols-auto">  
      <div class="col-2 people-name"><a target="_blank" href="https://www.iiitd.ac.in/jainendra">Jainendra Shukla</a><h6>IIIT Delhi</h6></div>
      <div class="col-2 people-name"><a target="_blank" href="https://scholar.google.co.in/citations?user=blKxdioAAAAJ&hl=en">Neeru Dubey</a><h6>KTH Sweden</h6></div>
      <div class="col-2 people-name"><a target="_blank" href="https://www.linkedin.com/in/parul-gupta-014a72127/?originalSubdomain=in">Parul Gupta</a><h6>Monash University</h6></div>
      <div class="col-2 people-name"><a target="_blank" href="http://www.pwpatil.com/">Prashant Patil</a><h6>IIT Guwahati</h6></div>
      <div class="col-2 people-name"><a target="_blank" href="https://scholar.google.co.in/citations?user=HgX8wb8AAAAJ&hl=en">Shruti Shantiling Phutke</a><h6>Griffith University</h6></div>
      <div class="col-2 people-name"><a target="_blank" href="http://yorkeyao.cc/">Yue Yao</a><h6>Curtin University</h6></div>
      </div>
</div>

<be>

#### Registration
For workshop registration details and related information, please visit the ACM-MM 2024 main website.

#### Contact
Please contact me if you have any questions.
<br>
Email: shreya.ghosh@curtin.edu.au

Image Source: Wall-E 

