---
title: "A2I: Affective Artificial Intelligence (ICPR 2024)" 
date: 2024-04-04T00:06:56+08:00
---

<br>
<div class="row">
  <div>
    <p><center>
        <img class="img-fluid banner-pic" src="/2024/A2I_banner.PNG">
    </center></p>
    <p><center>
      Full-day, <font size="3" color="red"> 1 Dec 2024, Kolkata</font> 
    </center></p>
  </div>
</div><br>


## Introduction

The workshop on Affective Artificial Intelligence (A2I) at ICPR 2024 aims to encourage and highlight state-of-the-art research in affective computing and applications. The key focuses are on novel neural network architectures, incorporating anatomical insights and constraints, introducing new and challenging datasets, and exploiting multi-modal training. 

## Call for Contributions

#### Full Workshop Papers
The workshop topics include (but are not limited to): 

- Large-scale data generation or Inexpensive annotation for Affective Computing
- AI methods for Affective Computing with multimodal data
- Multi-modal method for emotion recognition
- Explainable and/or Privacy Preserving AI in affective computing
- Generative and responsible personalization of affective phenomena estimators with few-shot learning
- Bias in affective computing data (e.g. lack of multi-cultural datasets)
- Semi-/weak-/un-/self- supervised learning methods, domain adaptation methods, and other novel methods for Affective Computing
- Applications in education, entertainment & healthcare 

<br>

We will be accepting the submission of full unpublished and original papers. These papers will be peer-reviewed via a single-blind process. The accepted papers in the workshops will be published in Lecture Notes in Computer Science (LNCS), Springer (<a href="https://www.springer.com/gp/computer-science/lncs" target="_blank">https://www.springer.com/gp/computer-science/lncs</a>). Please note that it will be published after the workshop.

#### Submission 
We invite authors to submit unpublished papers (15 pages including references <a href="https://icpr2024.org/cfp.html" target="_blank">ICPR format</a>) to our workshop, to be presented at an oral/poster session upon acceptance. All submissions will go through a single-blind review process. All contributions must be submitted (along with supplementary materials, if any) at the <a href = "https://cmt3.research.microsoft.com/A2IICPR2024" target="_blank">Microsoft CMT</a>. 

#### Note
Authors of previously rejected main conference submissions are also welcome to submit their work to our workshop. When doing so, you must submit the previous reviewers' comments (named as previous\_reviews.pdf) and a letter of changes (named as letter\_of\_changes.pdf) as part of your supplementary materials to clearly demonstrate the changes made to address the comments made by previous reviewers.
## Important Dates


<table class="table table-striped">
    <tbody>
        <tr>
          <td>Paper Submission Deadline</td>
          <td> <s>August 15, 2024</s> Sep 1, 2024 (11:55 pm Anywhere on Earth)</td>
        </tr>
        <tr>
          <td>Notification to Authors</td>
          <td>September 15, 2024 (11:55 pm Anywhere on Earth) </td>
        </tr>
        <tr>
          <td>Camera-Ready Deadline</td>
          <td>Please note that you need to upload your camera-ready version and copyright of your paper in a link provided by Springer. Springer will communicate with the contact author directly via email. (This is expected around first or second week of Nov 2024. We will update you around this time.)</td>
        </tr>
    </tbody>
</table>

## Workshop Schedule (Tentative)
#### Sunday, Dec 1st
Time zone:	IST [GMT + 5:30](https://www.timeanddate.com/time/zone/india/kolkata)

Location: [Biswa Bangla Convention Centre, Kolkata, India](https://icpr2024.org/venue.html).

More details: [ICPR full-program page](https://icpr2024.org/Pre-conference-event.html)

<table class="table table-striped">
    <tbody>
        <tr>
          <td> 9:30 - 9:35 </td>
          <td>Opening and welcome</td>
        </tr>
        <tr>
          <td>9:35 - 10:05</td>
          <td>Keynote 1: Multimodal and Context-Aware Emotion Perception by Prof. Dinesh Manocha</td>
        </tr>
        <tr>
          <td>10:05 - 10:20</td>
          <td>Paper 1: Detection and Analysis of Autism Spectrum Disorder Using Random Forest Classifier </td>
        </tr> 
      <tr>
          <td>10:20 - 10: 35</td>
          <td>Paper 2: Echo-GRU: Emotion Recognition Using Wearable EEG Supporting Early Alzheimer's Disease Detection	</td>
      </tr>
      <tr>
        <td>10:35 - 11:05</td>
        <td>Keynote 2: Learning Human Visual Attention and Gaze Communication Behaviours with Applications to Autism Spectrum Disorder Diagnosis by Prof. Arcot Sowmya</td>
      </tr>
      <tr>
          <td>11:05 - 11:30</td>
          <td>Break (Morning Tea)</td>
        </tr>
      <tr>
          <td>11:30 - 12:00</td>
          <td> Keynote 3: Uncovering Acoustic and Linguistic Behaviour for Autism Identification among Hindi-Speaking Children by Prof. Jainendra Shukla </td>
        </tr>
       <tr>
          <td>12:00 - 12:15</td>
          <td>Paper 3: Misinformation detection through multimodal machine learning analysis </td>
        </tr>
         <tr>
          <td>12:15 - 12: 30</td>
          <td>Paper 4: Exploring CNN Model Optimization Strategies for Plant Disease Classification </td>
        </tr>
      <tr>
          <td>12:30 - 12:45</td>
          <td>Paper 5: A Deep Learning Approach for Early Stress Detection Using Electrodermal Activity through Wearable Devices </td>
      </tr>
         <tr>
          <td>12:45 - 13:00</td>
          <td>Paper 6: Exploiting Automatic Source Separation and Music Transcription for Music Emotion Recognition </td>
        </tr>       
       <tr>
          <td>13:00 - 13:30</td>
          <td>Keynote 4: Unravelling the Importance of Remote Photoplethysmography (rPPG) in Affective Computing by Prof. Puneet Gupta</td>
        </tr>
      <tr>
          <td>13:30 - 14:30</td>
          <td>Lunch break</td>
        </tr>      
      <tr>
          <td>14:30 - 14:45</td>
          <td>Paper 7: Emotion Pattern Recognition of Speech Signals using Representation Learning with Limited Annotations </td>
        </tr>      
      <tr>
          <td>14: 45 - 15:00</td>
          <td>Paper 8: Can AI Decode the Circumplex Model of Affect ? A Data-Driven Study </td>
        </tr>      
      <tr>
          <td>15:00 - 15:15</td>
          <td>Paper 9: A Spiking Neural Network Framework for Classifying Ictal and Interictal Epileptic States </td>
        </tr>      
      <tr>
          <td>15:30 - 16:00</td>
          <td>Keynote 5: Designing Behaviorally-Intelligent Agents: Modeling Motion, Interactions, and Collaborations by Prof. Aniket Bera</td>
        </tr>      
      <tr>
          <td>16:00 - 16:30</td>
          <td>Break (Tea break)</td>
        </tr>
<tr>
          <td>16:30 - 17:00</td>
          <td>Keynote 6: Integrating Micro-Emotion Recognition with Mental Health Estimation for Improved Well-being by Prof. Santosh Vipparthi</td>
        </tr>
        <tr>
          <td>17:00 - 17:05</td>
          <td>Closing Remarks</td>
        </tr>
    </tbody>
</table>


## Invited Keynote Speakers
<div class="row">
    <div class="col-3">
        <a href="https://www.cs.umd.edu/people/dmanocha">
            <img class="speaker-pic pull-right" src="/2024/dm2018-face.jpg" />
        </a>
    </div>
    <div class="col">
        <a href="https://www.cs.umd.edu/people/dmanocha"><span class="fs-3">Prof. Dinesh Manocha<span class="fs-3"></span></a>
        <h6 class="fs-5">University of Maryland</h6>
          <Strong>Title:</Strong> Multimodal and Context-Aware Emotion Perception <br>          
          <p align="justify"><Strong>Abstarct:</Strong> Human emotion perception is integral to intelligent systems' wide applications, including behavior prediction, social robotics, medicine, surveillance, and entertainment. Current literature advocates that humans perceive emotions and behavior from various human modalities and situational and background contexts. Our research focuses on this aspect of emotion perception, as we attempt to build emotion perception models from multiple modalities and contextual cues and use such ideas of perception for various real-world domains of AI applications. We will go over both parts in this talk. In the first part, we will explore two approaches to improve emotion perception models. In one approach, we will leverage the idea of using more than one modality to perceive human emotion. In the other approach, we leverage contextual information; background scene, multiple modalities of the human subject, and socio-dynamic inter-agent interactions available in the input to predict the perceived emotion. In the second part, we will explore three domains of AI applications; i) video manipulations and deepfake detection, ii) multimedia content analysis, and iii) social media interactions investigation to enrich solutions with ideas from emotion perception.</p> <br>          
      <p align="justify"><Strong>Biography:</Strong> Prof. Dinesh Manocha is Paul Chrisman-Iribe Chair in Computer Science & ECE and a Distinguished University Professor at the University of Maryland College Park. His research interests include virtual environments, audio, physically-based modeling, and robotics. His group has developed multiple software packages that are standard and licensed to 60+ commercial vendors. He has published more than 790 papers & supervised 50 PhD dissertations. He is a Fellow of AAAI, AAAS, ACM, IEEE, and NAI, a member of ACM SIGGRAPH and IEEE VR Academies, and a Bézier Award from the Solid Modeling Association. He received the Distinguished Alumni Award from IIT Delhi and the Distinguished Career in Computer Science Award from the Washington Academy of Sciences. He was a co-founder of Impulsonic, a developer of physics-based audio simulation technologies, which Valve Inc acquired in November 2016.  
     </p> 
    </div>
</div>
<br>
<br>
<div class="row">
    <div class="col-3">
        <a href="https://www.cs.purdue.edu/homes/ab/"> 
            <img class="speaker-pic pull-right" src="/2024/ab.jpg" />
        </a>
    </div> 
    <div class="col">
        <a href="https://www.cs.purdue.edu/homes/ab/"><span class="fs-3">A/Prof. Aniket Bera</span></a>
        <h6 class="fs-5">Purdue University</h6>
      <Strong>Title:</Strong> Designing Behaviorally-Intelligent Agents: Modeling Motion, Interactions, and Collaborations <br>          
          <p align="justify"><Strong>Abstarct:</Strong> Recent advances in Robotic technologies are gradually enabling humans and robots to co-exist, co-work, and share spaces in different environments. Robots are increasingly required to navigate in socially-acceptable yet collision-free paths in a crowd in places such as campuses, airports, and shopping malls; to interact and understand people in their homes, workplaces, and hospitals; and to share responsibility for completing tasks, meaning that robots are becoming social partners and teammates with humans. In the future, these socially intelligent robots will likely enter almost all human domains, including healthcare facilities, factories, airports, warehouses, schools, etc.

In this talk, we will focus on our research on the development of autonomous systems to navigate complex and dynamic environments with a high degree of situational awareness and human-like adaptability. This work leverages deep learning and geometric reasoning to create behavior-aware navigation strategies, allowing robots to predict and respond to human movement patterns in real-time.
</p> <br>          
<p align="justify"><Strong>Biography:</Strong> Prof. Aniket Bera is an Associate Professor at the Department of Computer Science at Purdue University. He directs the interdisciplinary research lab IDEAS (Intelligent Design for Empathetic and Augmented Systems) at Purdue, working on modeling the "human" and "social" aspects using AI in Robotics, Graphics, and Vision. He is also an Adjunct Associate Professor at the University of Maryland at College Park. Prior to this, he was a Research Assistant Professor at the University of North Carolina at Chapel Hill. He received his Ph.D. in 2017 from the University of North Carolina at Chapel Hill.  He is currently serving as the Senior Editor for IEEE Robotics and Automation Letters (RA-L) in the area of "Planning and Simulation" and the Conference Chair for the ACM SIGGRAPH Conference on Motion, Interaction and Games (MIG 2022) and Outreach Chair for 22nd ACM SIGGRAPH/EUROGRAPHICS Symposium for Computer Animation (SCA 2023). His core research interests are in Affective Computing, Computer Graphics (AR/VR, Augmented Intelligence, Multi-Agent Simulation), Social Robotics, Autonomous Agents, Cognitive modeling, and planning for intelligent characters. He has advised and co-advised multiple M.S. and Ph.D. students. His work has won multiple best paper awards at top Graphics/VR conferences. He also works with the University of Maryland at Baltimore Medical School to build algorithms and systems to help therapists and doctors detect mental health and social anxiety issues. His research involves novel combinations of methods and collaborations in machine learning, computational psychology, computer graphics, and physically-based simulation to develop real-time computational models to learn human behaviors. Dr. Bera has previously worked in many research labs, including Disney Research, Intel, and the Centre for Development of Advanced Computing. Dr. Bera's research has been featured on CBS, WIRED, Forbes, FastCompany, NPR, etc. </p>
    </div>
</div>
<be>

<br>
<div class="row">
    <div class="col-3">
        <a href="https://www.iiitd.ac.in/jainendra"> 
            <img class="speaker-pic pull-right" src="/2024/js.jpg" />
        </a>
    </div> 
    <div class="col">
        <a href="https://www.iiitd.ac.in/jainendra"><span class="fs-3">Prof. Jainendra Shukla</span></a>
        <h6 class="fs-5">IIIT-Delhi</h6>
      <Strong>Title:</Strong> Uncovering Acoustic and Linguistic Behaviour for Autism Identification among Hindi-Speaking Children <br>          
          <p align="justify"><Strong>Abstarct:</Strong> The identification of Autism Spectrum Disorder (ASD) faces challenges due to the lack of reliable biomarkers and the subjectivity in diagnostic procedures, necessitating improved tools for objectivity and efficiency. Being a key characteristic of autism, language impairments are regarded as potential markers for identifying ASD. However, current research predominantly focuses on analyzing language characteristics in English, overlooking linguistic and contextual specificities in other resource constrained languages. Motivated by these, In collaboration with AIIMS, New Delhi, we developed an Artificial Intelligence (AI)- based system to detect ASD, utilizing a range of acoustic and linguistic features extracted from dyadic conversations between a child and their communication partner. Validating our model on 76 English-speaking children [35 ASD and 41 typically developing (TD) ] and 33 Hindi-speaking children (15 with ASD and 18 TD), our extensive analysis of a diverse and comprehensive set of acoustic and linguistic speech attributes, including lexical, syntactic, semantic, and pragmatic elements revealed reliable speech attributes as potential predictors of ASD. We further addressed the influence of linguistic diversity on speech-based ASD assessment by examining speech behaviours in both English and the low-resource language, Hindi. This study underscores the reliability of speech-based biomarkers in ASD assessment, emphasizing their effectiveness across diverse linguistic backgrounds and highlighting the need for language-specific research in this domain. </p>
          <br>
<p align="justify"><Strong>Biography:</Strong> Dr. Jainendra Shukla, an Associate Professor in the Department of Computer Science and Engineering at IIIT-Delhi, is the founder and director of the Human-Machine Interaction (HMI) research group. He is enthusiastic about empowering machines with emotional intelligence and adaptive interaction ability that can improve the quality of life in health and social care. Dr. Shukla leads the EU-funded IRAS-HUB project, aimed at advancing robotics training in India, and co-developed India’s first MOOC on Affective Computing. He serves as an Associate Editor for IEEE Transactions on Affective Computing and has published in top venues like CHI, UbiComp, and IEEE Transactions. Recognized with honors such as SIGCHI's Honorable Mention (CHI’24) and multiple research and teaching awards, his contributions are shaping the future of human-machine interaction. </p>
    </div>
</div>
<br>

<br>
<div class="row">
    <div class="col-3">
        <a href="https://www.iiti.ac.in/people/~puneet/"> 
            <img class="speaker-pic pull-right" src="/2024/pg.jpg" />
        </a>
    </div> 
    <div class="col">
        <a href="https://www.iiti.ac.in/people/~puneet/"><span class="fs-3">Prof. Puneet Gupta</span></a>
        <h6 class="fs-5">IIT Indore</h6>
         <p align="justify"> <Strong>Title:</Strong> Unravelling the Importance of Remote Photoplethysmography (rPPG) in Affective Computing </p>
      <p align="justify"> <Strong> Abstract:</Strong> In the rapidly evolving field of affective computing, the integration of Remote Photoplethysmography (rPPG) has emerged as an innovative technique for non-invasive physiological monitoring. This keynote addresses the pivotal role of rPPG in advancing our understanding and application of affective computing. Remote Photoplethysmography (or rPPG) is a technique that enables the measurement of blood volume changes in the skin using camera-based systems, providing a contactless method to capture physiological signals such as heart rate and respiratory rate. By analyzing subtle changes in skin colour, rPPG offers a unique window into the autonomic nervous system's responses, making it an invaluable tool for emotion recognition and mental health assessment. This talk will delve into the core principles of rPPG and explore its diverse applications within affective computing. We will examine how rPPG can be utilized to detect facial microexpressions, which are critical for understanding genuine emotional states and enhancing lie detection systems.
<br>
Additionally, we will discuss the application of rPPG in stress detection, providing insights into real-time monitoring of stress levels in various environments, from workplaces to daily life scenarios. Moreover, the keynote will highlight the potential of rPPG in the early detection and monitoring of depression, offering a non-intrusive method to support mental health professionals. We will also explore its use in other domains, such as anxiety detection, fatigue monitoring, and enhancing user experience in human-computer interaction. By unravelling the importance of rPPG in affective computing, this keynote aims to underscore the transformative impact of this technology on both research and practical applications, paving the way for more sophisticated, empathetic, and responsive systems.</p> <br>
<p align="justify"> <Strong>Biography:</Strong> Dr. Puneet Gupta is currently working as an Associate Professor with the Department of Computer Science and Engineering, at the Indian Institute of Technology (IIT) Indore. His broad research interests include Computer Vision, Deep Learning, and Image Processing. He works to make the current technology useful for human beings by analyzing their behavior. He has worked on fusing multiple biometric traits for authentication; analyzing facial expressions using deep learning; measuring the human-vitals (which are, heart rate, breathing rate and blood pressure) using unobtrusive and non-contact human videos; and cross-modal learning. These play indispensable role in security; affective computing; ambient intelligence; and health-care.</p>
    </div>
</div>
<br>
<div class="row">
    <div class="col-3">
        <a href="https://research.unsw.edu.au/people/professor-arcot-sowmya"> 
            <img class="speaker-pic pull-right" src="/2024/as.jpg" />
        </a>
    </div> 
    <div class="col">
        <a href="https://research.unsw.edu.au/people/professor-arcot-sowmya"><span class="fs-3">Prof. Arcot Sowmya</span></a>
        <h6 class="fs-5">UNSW Sydney</h6>
    <p align="justify"> <Strong>Title:</Strong> Title: Learning Human Visual Attention and Gaze Communication Behaviours with Applications to Autism Spectrum Disorder Diagnosis </p>
      <p align="justify"> <Strong> Abstract:</Strong> Humans employ a complex combination of verbal and non-verbal communication to convey information and express intentions. While the former is extensively used during communication, the latter is just as important. In fact, non-verbal communication such as visual attention and gaze communication behaviours contain a wealth of information about a person’s affective, cognitive and mental states. The limited ability to communicate effectively hinders knowledge acquisition and are noticeable in mental health disorders such as autism spectrum disorders (ASD). There are currently no medical tests that diagnose ASD. Instead, clinicians employ a combination of a comprehensive analysis of developmental history and manual observation of atypical behaviours to diagnose a patient. It is vital to develop new diagnostic methods to ensure that early interventions are provided to optimise health outcomes. In this talk, I will outline our work on developing computational models that aim to mimic and/or quantify visual attention and gaze communication behaviours of neurotypical groups. These models are then trained and/or fine-tuned on datasets that involve neurotypical and ASD individuals and used for ASD diagnosis and severity prediction. Other potential applications of this tool include general human behaviour understanding, digital behaviour phenotyping and human-computer interaction.
<p align="justify"> <Strong>Biography:</Strong> Arcot Sowmya is Professor and Head of School, School of Computer Science and Engineering at the University of New South Wales. She obtained her PhD in Computer Science and Engineering from the Indian Institute of Technology, Bombay. Her research utilises and extends powerful techniques drawn from machine learning and pattern recognition and applies them to the complex problems of learning models for segmentation, classification, recognition and prediction in high-resolution images, including satellite and aerial images, medical images and multimodal datasets including omics data, and motion segmentation and classification in video images. Her research is supported by competitive and industry grants and has led to technology transfer to industry multiple times. She was the Winner of the 2023Telstra Brilliant Women in Digital Health Award for Research.</p>
    </div>
</div>
<br>
<div class="row">
    <div class="col-3">
        <a href="https://skvipparthi.github.io/"> 
            <img class="speaker-pic pull-right" src="/2024/skv.jpg" />
        </a>
    </div> 
    <div class="col">
        <a href="https://skvipparthi.github.io/"><span class="fs-3">Prof. Santosh Kumar Vipparthi</span></a>
        <h6 class="fs-5">IIT Ropar</h6>
    <p align="justify"> <Strong>Title:</Strong> Integrating Micro-Emotion Recognition with Mental Health Estimation for Improved Well-being </p>
      <p align="justify"> <Strong> Abstract:</Strong> Micro-emotion recognition, through subtle cues like micro-expressions, offers critical insights into an individual’s emotional state, essential for mental health assessment. Micro-expressions, brief and involuntary, reveal hidden emotions but are challenging to detect due to their subtle nature. This research proposes a novel method to capture and analyze micro-expressions for mental health applications by preserving essential facial movements in a single video frame.

The Lateral Accretive Hybrid Network (LEARNet) is introduced to extract micro-level emotional features, refining both broad and subtle facial cues that can indicate mental health conditions like anxiety or depression. Additionally, the authors propose an efficient neural architecture search (NAS) strategy to design a compact, effective CNN for micro-expression recognition, improving performance and resource use.

This approach integrates micro-emotion recognition with mental health estimation, enabling more accurate and early detection of emotional and mental health issues, ultimately leading to improved well-being and mental health monitoring.
 </p>
<p align="justify"> <Strong>Biography:</Strong> Dr. Santosh Kumar Vipparthi is an Associate Professor at the School of Artificial Intelligence and Data Engineering, Indian Institute of Technology Ropar (IIT Ropar). Previously, he was with the Indian Institute of Technology Guwahati (IIT Guwahati). With over eleven years of post-PhD research experience, Dr. Vipparthi has made significant contributions to the fields of human emotion recognition, surveillance across various mediums, and artificial intelligence. </p>
    </div>
</div>
<br>
<style>
    .speaker-pic {
        width: 250px;
        height: 250px;
    }
</style>

## Organizers

<div class="container p-0">
  <div class="row">
    <div class="col">
        <a href="https://sanjay-ghosh.github.io/">
            <img class="organizer-pic" src="/2024/sg.jpg">
        </a>
        <div class="people-name orgnizer-people-name">
            <a href="https://sanjay-ghosh.github.io/">Sanjay Ghosh</a>
            <h6 class="uni-name">IIT Kharagpur</h6>
        </div>
    </div>
    <div class="col">
        <a href="https://staffportal.curtin.edu.au/staff/profile/view/shreya-ghosh-a2f9d3ca/">
            <img class="organizer-pic" src="/2023/img/people/ShreyaGhosh.jpg"/> 
        </a>
        <div class="people-name orgnizer-people-name">
            <a href="https://staffportal.curtin.edu.au/staff/profile/view/shreya-ghosh-a2f9d3ca/">Shreya Ghosh</a>
            <h6 class="uni-name">Curtin University</h6>
        </div>
    </div>
    <div class="col">
        <a href="https://www.flinders.edu.au/people/abhinav.dhall">
            <img class="organizer-pic" src="/2023/img/people/AbhinavDhall.jpg">
        </a>
        <div class="people-name orgnizer-people-name">
            <a href="https://www.flinders.edu.au/people/abhinav.dhall">Abhinav Dhall</a>
            <h6 class="uni-name">Flinders University</h6>
        </div>
    </div>
   <div class="col">
        <a href="https://staffportal.curtin.edu.au/staff/profile/view/tom-gedeon-5e48a1fd/">
            <img class="organizer-pic" src="/2023/img/people/TomGedeon.jpg">
        </a>
        <div class="people-name orgnizer-people-name">
            <a href="https://staffportal.curtin.edu.au/staff/profile/view/tom-gedeon-5e48a1fd/">Tom Gedeon</a>
            <h6 class="uni-name">Curtin University</h6>
        </div>
    </div>
  </div>
</div>

<style>
.organizer-pic {
    width: 200px;
    height: 200px;
}
.uni-name {
    max-width: 200px
}

.people-name {
    max-width: 200px;
} 

.orgnizer-people-name {
    text-align: center;
}

.speaker-pic, .organizer-pic {
    border-radius: 50%;
}

.banner-pic {
    width: 900px;
    height: auto;
}
</style>

## Program Committee

<div class="container p-0">
    <div class="row row-cols-auto">  
      <div class="col-2 people-name"><a target="_blank" href="https://www.linkedin.com/in/lownish-rai-sookha-220294149/?originalSubdomain=in">Lownish Sookha</a><h6>IIT Ropar</h6></div> 
      <div class="col-2 people-name"><a target="_blank" href="http://www.pwpatil.com/">Prashant Patil</a><h6>IIT Guwahati</h6></div>
      <div class="col-2 people-name"><a target="_blank" href="https://scholar.google.co.in/citations?user=HgX8wb8AAAAJ&hl=en">Shruti Shantiling Phutke</a><h6>Griffith University</h6></div>
      <div class="col-2 people-name"><a target="_blank" href="http://yorkeyao.cc/">Yue Yao</a><h6>Curtin University</h6></div>
      <div class="col-2 people-name"><a target="_blank" href="https://hasan-rakibul.github.io/">Rakibul Hasan</a><h6>Curtin University</h6></div>
      <div class="col-2 people-name"><a target="_blank" href="https://www.linkedin.com/in/surbhi-madan-3384b130/?originalSubdomain=in">Surbhi Madan</a><h6>IIT Ropar</h6></div>
      <div class="col-2 people-name"><a target="_blank" href="https://sites.google.com/view/gulshansharma">Gulshan Sharma</a><h6>IIT Ropar</h6></div>
      <div class="col-2 people-name"><a target="_blank" href="https://www.linkedin.com/in/hrishav-bakul-barua-2b9b1765/">Hrishav Bakul Barua</a><h6>Monash University</h6></div>
      <div class="col-2 people-name"><a target="_blank" href="https://deepakkumar-iitr.github.io/">Deepak Kumar</a><h6>IIT Roorkee</h6></div>
      <div class="col-2 people-name"><a target="_blank" href="https://www.linkedin.com/in/ruining-yang/">Ruining Yang</a><h6>Northeastern University</h6></div>
      <div class="col-2 people-name"><a target="_blank" href="https://www.linkedin.com/in/akankshachuchra/">Akanksha Chuchra</a><h6>IIT Ropar</h6></div>
      <div class="col-2 people-name"><a target="_blank" href="">Baibei Ji</a><h6>Soochow University</h6></div>
      <div class="col-2 people-name"><a target="_blank" href="https://www.linkedin.com/in/phyo-thet-yee-7a54001b6/">Phyo Yee</a><h6>IIT Ropar</h6></div>


</div>

<be>

#### Registration
Workshop registration will be handled by ICPR-2024 main conference committee. Please follow ICPR-2024 website for related information.

#### Contact

<!--<div class="container p-0">
    <div class="row row-cols-auto">
        <div class="col-2 people-name"><a target="_blank" href="https://staffportal.curtin.edu.au/staff/profile/view/shreya-ghosh-a2f9d3ca/">Shreya Ghosh</a><h6>Curtin University</h6></div>
</div>-->

Please contact us if you have any questions.
<br>
Email: sanjay.ghosh@ee.iitkgp.ac.in, shreya.ghosh@curtin.edu.au, abhinav.dhall@flinders.edu.au

Image Source: [Medium](https://cdn-images-1.medium.com/v2/resize:fit:720/1*iikDESiXRDRt6ycbrX6Q7A.jpeg)  


