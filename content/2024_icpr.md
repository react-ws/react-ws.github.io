---
title: "A2I: Affective Artificial Intelligence (ICPR 2024)" 
date: 2024-04-04T00:06:56+08:00
---

<br>
<div class="row">
  <div>
    <p><center>
        <img class="img-fluid banner-pic" src="/2024/A2I_banner.PNG">
    </center></p>
    <p><center>
      Full-day, <font size="3" color="red"> Dec 2024, Kolkata</font> 
    </center></p>
  </div>
</div><br>


## Introduction

The workshop on Affective Artificial Intelligence (A2I) at ICPR 2024 aims to encourage and highlight state-of-the-art research in affective computing and applications. The key focuses are on novel neural network architectures, incorporating anatomical insights and constraints, introducing new and challenging datasets, and exploiting multi-modal training. 

## Call for Contributions

#### Full Workshop Papers
The workshop topics include (but are not limited to): 

- Large-scale data generation or Inexpensive annotation for Affective Computing
- AI methods for Affective Computing with multimodal data
- Multi-modal method for emotion recognition
- Explainable and/or Privacy Preserving AI in affective computing
- Generative and responsible personalization of affective phenomena estimators with few-shot learning
- Bias in affective computing data (e.g. lack of multi-cultural datasets)
- Semi-/weak-/un-/self- supervised learning methods, domain adaptation methods, and other novel methods for Affective Computing
- Applications in education, entertainment & healthcare 

<br>

We will be accepting the submission of full unpublished and original papers. These papers will be peer-reviewed via a single-blind process. The accepted papers in the workshops will be published in Lecture Notes in Computer Science (LNCS), Springer (<a href="https://www.springer.com/gp/computer-science/lncs" target="_blank">https://www.springer.com/gp/computer-science/lncs</a>). Please note that it will be published after the workshop.

#### Submission 
We invite authors to submit unpublished papers (15 pages including references <a href="https://icpr2024.org/cfp.html" target="_blank">ICPR format</a>) to our workshop, to be presented at an oral/poster session upon acceptance. All submissions will go through a single-blind review process. All contributions must be submitted (along with supplementary materials, if any) at the <a href = "https://cmt3.research.microsoft.com/A2IICPR2024" target="_blank">Microsoft CMT</a>. 

#### Note
Authors of previously rejected main conference submissions are also welcome to submit their work to our workshop. When doing so, you must submit the previous reviewers' comments (named as previous\_reviews.pdf) and a letter of changes (named as letter\_of\_changes.pdf) as part of your supplementary materials to clearly demonstrate the changes made to address the comments made by previous reviewers.
## Important Dates


<table class="table table-striped">
    <tbody>
        <tr>
          <td>Paper Submission Deadline</td>
          <td>August 15, 2024 (11:55 pm Anywhere on Earth)</td>
        </tr>
        <tr>
          <td>Notification to Authors</td>
          <td>September 10, 2024 (11:55 pm Anywhere on Earth) </td>
        </tr>
        <tr>
          <td>Camera-Ready Deadline</td>
          <td>September 20, 2024 (11:55 pm Anywhere on Earth)</td>
        </tr>
    </tbody>
</table>

## Workshop Schedule
TBD

## Invited Keynote Speakers
<div class="row">
    <div class="col-3">
        <a href="https://www.cs.umd.edu/people/dmanocha">
            <img class="speaker-pic pull-right" src="/2024/dm2018-face.jpg" />
        </a>
    </div>
    <div class="col">
        <a href="https://www.cs.umd.edu/people/dmanocha"><span class="fs-3">Prof. Dinesh Manocha<span class="fs-3"></span></a>
        <h6 class="fs-5">University of Maryland</h6>
          <Strong>Title:</Strong> Multimodal and Context-Aware Emotion Perception <br>          
          <Strong>Abstarct:</Strong> Human emotion perception is integral to intelligent systems' wide applications, including behavior prediction, social robotics, medicine, surveillance, and entertainment. Current literature advocates that humans perceive emotions and behavior from various human modalities and situational and background contexts. Our research focuses on this aspect of emotion perception, as we attempt to build emotion perception models from multiple modalities and contextual cues and use such ideas of perception for various real-world domains of AI applications. We will go over both parts in this talk. In the first part, we will explore two approaches to improve emotion perception models. In one approach, we will leverage the idea of using more than one modality to perceive human emotion. In the other approach, we leverage contextual information; background scene, multiple modalities of the human subject, and socio-dynamic inter-agent interactions available in the input to predict the perceived emotion. In the second part, we will explore three domains of AI applications; i) video manipulations and deepfake detection, ii) multimedia content analysis, and iii) social media interactions investigation to enrich solutions with ideas from emotion perception. <br>          
      <Strong>Biography:</Strong> Prof. Dinesh Manocha is Paul Chrisman-Iribe Chair in Computer Science & ECE and a Distinguished University Professor at the University of Maryland College Park. His research interests include virtual environments, audio, physically-based modeling, and robotics. His group has developed multiple software packages that are standard and licensed to 60+ commercial vendors. He has published more than 790 papers & supervised 50 PhD dissertations. He is a Fellow of AAAI, AAAS, ACM, IEEE, and NAI, a member of ACM SIGGRAPH and IEEE VR Academies, and a Bézier Award from the Solid Modeling Association. He received the Distinguished Alumni Award from IIT Delhi and the Distinguished Career in Computer Science Award from the Washington Academy of Sciences. He was a co-founder of Impulsonic, a developer of physics-based audio simulation technologies, which Valve Inc acquired in November 2016.      
    </div>
</div>
<br>
<br>
<div class="row">
    <div class="col-3">
        <a href="https://www.cs.purdue.edu/homes/ab/"> 
            <img class="speaker-pic pull-right" src="/2024/ab.jpg" />
        </a>
    </div> 
    <div class="col">
        <a href="https://www.cs.purdue.edu/homes/ab/"><span class="fs-3">Dr. Aniket Bera</span></a>
        <h6 class="fs-5">Purdue University</h6>
         <Strong>Biography:</Strong> Dr. Aniket Bera is an Associate Professor at the Department of Computer Science at Purdue University. He directs the interdisciplinary research lab IDEAS (Intelligent Design for Empathetic and Augmented Systems) at Purdue, working on modeling the "human" and "social" aspects using AI in Robotics, Graphics, and Vision. He is also an Adjunct Associate Professor at the University of Maryland at College Park. Prior to this, he was a Research Assistant Professor at the University of North Carolina at Chapel Hill. He received his Ph.D. in 2017 from the University of North Carolina at Chapel Hill. He is also the founder of Project Dost. He is currently serving as the Senior Editor for IEEE Robotics and Automation Letters (RA-L) in the area of "Planning and Simulation" and the Conference Chair for the ACM SIGGRAPH Conference on Motion, Interaction and Games (MIG 2022). His core research interests are in Affective Computing, Computer Graphics (AR/VR, Augmented Intelligence, Multi-Agent Simulation), Social Robotics, Autonomous Agents, Cognitive modelling, and planning for intelligent characters. He has advised and co-advised multiple M.S. and Ph.D. students. He has authored over 70+ papers, 2000+ citations and his work has won multiple awards at top Graphics/VR conferences. He also works with the University of Maryland at Baltimore Medical School to build algorithms and systems to help therapists and doctors detect mental health and social anxiety issues (AI + Mental Health). His research involves novel combinations of methods and collaborations in machine learning, computational psychology, computer graphics, and physically-based simulation to develop real-time computational models to learn human behaviours. Dr. Bera has previously worked in many research labs, including Disney Research, Intel, and the Centre for Development of Advanced Computing. Dr. Bera's research has been featured on CBS, WIRED, Forbes, FastCompany, Times of India etc.
    </div>
</div>

<br>

<style>
    .speaker-pic {
        width: 250px;
        height: 250px;
    }
</style>

## Organizers

<div class="container p-0">
  <div class="row">
    <div class="col">
        <a href="https://sanjay-ghosh.github.io/">
            <img class="organizer-pic" src="/2024/sg.jpg">
        </a>
        <div class="people-name orgnizer-people-name">
            <a href="https://sanjay-ghosh.github.io/">Sanjay Ghosh</a>
            <h6 class="uni-name">IIT Kharagpur</h6>
        </div>
    </div>
    <div class="col">
        <a href="https://staffportal.curtin.edu.au/staff/profile/view/shreya-ghosh-a2f9d3ca/">
            <img class="organizer-pic" src="/2023/img/people/ShreyaGhosh.jpg"/> 
        </a>
        <div class="people-name orgnizer-people-name">
            <a href="https://staffportal.curtin.edu.au/staff/profile/view/shreya-ghosh-a2f9d3ca/">Shreya Ghosh</a>
            <h6 class="uni-name">Curtin University</h6>
        </div>
    </div>
    <div class="col">
        <a href="https://www.flinders.edu.au/people/abhinav.dhall">
            <img class="organizer-pic" src="/2023/img/people/AbhinavDhall.jpg">
        </a>
        <div class="people-name orgnizer-people-name">
            <a href="https://www.flinders.edu.au/people/abhinav.dhall">Abhinav Dhall</a>
            <h6 class="uni-name">Flinders University</h6>
        </div>
    </div>
   <div class="col">
        <a href="https://staffportal.curtin.edu.au/staff/profile/view/tom-gedeon-5e48a1fd/">
            <img class="organizer-pic" src="/2023/img/people/TomGedeon.jpg">
        </a>
        <div class="people-name orgnizer-people-name">
            <a href="https://staffportal.curtin.edu.au/staff/profile/view/tom-gedeon-5e48a1fd/">Tom Gedeon</a>
            <h6 class="uni-name">Curtin University</h6>
        </div>
    </div>
  </div>
</div>

<style>
.organizer-pic {
    width: 200px;
    height: 200px;
}
.uni-name {
    max-width: 200px
}

.people-name {
    max-width: 200px;
} 

.orgnizer-people-name {
    text-align: center;
}

.speaker-pic, .organizer-pic {
    border-radius: 50%;
}

.banner-pic {
    width: 900px;
    height: auto;
}
</style>

## Program Committee (To be updated)

<div class="container p-0">
    <div class="row row-cols-auto">  
<!--       <div class="col-2 people-name"><a target="_blank" href="https://www.iiitd.ac.in/jainendra">Jainendra Shukla</a><h6>IIIT Delhi</h6></div> -->
      <div class="col-2 people-name"><a target="_blank" href="https://scholar.google.co.in/citations?user=blKxdioAAAAJ&hl=en">Neeru Dubey</a><h6>KTH Sweden</h6></div>
      <div class="col-2 people-name"><a target="_blank" href="https://www.linkedin.com/in/parul-gupta-014a72127/?originalSubdomain=in">Parul Gupta</a><h6>Monash University</h6></div>
      <div class="col-2 people-name"><a target="_blank" href="http://www.pwpatil.com/">Prashant Patil</a><h6>IIT Guwahati</h6></div>
      <div class="col-2 people-name"><a target="_blank" href="https://scholar.google.co.in/citations?user=HgX8wb8AAAAJ&hl=en">Shruti Shantiling Phutke</a><h6>Griffith University</h6></div>
      <div class="col-2 people-name"><a target="_blank" href="http://yorkeyao.cc/">Yue Yao</a><h6>Curtin University</h6></div>
    </div>
</div>

<be>

#### Registration
Workshop registration will be handled by ICPR-2024 main conference committee. Please follow ICPR-2024 website for related information.

#### Contact

<!--<div class="container p-0">
    <div class="row row-cols-auto">
        <div class="col-2 people-name"><a target="_blank" href="https://staffportal.curtin.edu.au/staff/profile/view/shreya-ghosh-a2f9d3ca/">Shreya Ghosh</a><h6>Curtin University</h6></div>
</div>-->

Please contact us if you have any questions.
<br>
Email: sanjay.ghosh@ee.iitkgp.ac.in, shreya.ghosh@curtin.edu.au, abhinav.dhall@flinders.edu.au

Image Source: [Medium](https://cdn-images-1.medium.com/v2/resize:fit:720/1*iikDESiXRDRt6ycbrX6Q7A.jpeg)  


