---
title: "MRAC: Multimodal, Generative and Responsible Affective Computing (ACM-MM 2025)" 
date: 2025-04-04T00:06:56+08:00
---

<br>
<div class="row">
  <div>
<!--     <p><center>
        <img class="img-fluid banner-pic" src="/2024/MRAC_2024_banner.PNG"> (Morning Session, Meeting Room 217, Level 2 of Melbourne Convention and Exhibition Centre (MCEC)),
    </center></p>
    <p><center> -->
      Half-day  <font size="3" color="red"> 27 Oct 2025 1:30pm - 5:00pm Dublin Time, Ireland</font> 
    </center></p>
  </div>
</div><br>


## Introduction

Affective Computing involves the creation, evaluation and deployment of Emotion AI and Affective technologies to make peopleâ€™s lives better. The creation, evaluation and deployment stages of the emotion-ai model require large amounts of multimodal data from RGB images to video, audio, text, and physiological signals. In principle, the development of any AI system must be guided by a concern for its human impact. The aim should be striving to augment and enhance humans, not replace humans; while taking inspiration from human intelligence, safely. To this end, the MRAC 2025 workshop aims to transfer the same concepts from a small-scale, lab-based environment to a real-world, large-scale corpus enhanced with responsibility. The workshop also aims to bring to the attention of researchers and industry professionals of the potential implications of generative technology along with its ethical consequences. 


## Call for Contributions

### Full Workshop Papers

The 3rd International Workshop on Multimodal, Generative and Responsible Affective Computing (MRAC 2025) at <a href="https://acmmm2025.org/" target="_blank">ACM-MM 2025</a> (track for Multimodal and Responsible Affective Computing) aims to encourage and highlight novel strategies for affective phenomena estimation and prediction with a focus on robustness and accuracy in extended parameter spaces, spatially, temporally, spatio-temporally and most importantly Responsibly. This is expected to be achieved by applying novel neural network architectures, generative ai, incorporating anatomical insights and constraints, introducing new and challenging datasets, and exploiting multi-modal training. Specifically, the workshop topics include (but are not limited to):

- Large scale data generation or Inexpensive annotation for Affective Computing
- Generative AI for Affective Computing using multimodal signals
- Multi-modal method for emotion recognition
- Privacy preserving large scale emotion recognition in the wild
- Generative aspects of affect analysis
- Deepfake generation, detection and temporal deepfake localization
- Multimodal data analysis
- Affective Computing Applications in education, entertainment & healthcare
- Explainable or Privacy Preserving AI in affective computing
- Generative and responsible personalization of affective phenomena estimators with few-shot learning
- Bias in affective computing data (e.g. lack of multi-cultural datasets)
- Semi-/weak-/un-/self- supervised learning methods, domain adaptation methods, and other novel methods for Affective Computing 

<br>

We will be accepting the submission of full unpublished and original papers. These papers will be peer-reviewed via a double-blind process, and will be published in the official workshop proceedings and be presented at the workshop itself. 

#### Submission 
We invite authors to submit unpublished papers (<a href="https://acmmm2025.org/call-for-papers/" target="_blank">ACM-MM format</a>) to our workshop, to be presented at an oral/poster session upon acceptance. All submissions will go through a double-blind review process. All contributions must be submitted (along with supplementary materials, if any) at the <a href = "https://cmt3.research.microsoft.com/MRAC2025/Submission/Index" target="_blank">CMT</a>. Accepted papers will be published in the official ACM-MM Workshops proceedings.

#### Note
Authors of previously rejected main conference submissions are also welcome to submit their work to our workshop. When doing so, you must submit the previous reviewers' comments (named as previous\_reviews.pdf) and a letter of changes (named as letter\_of\_changes.pdf) as part of your supplementary materials to clearly demonstrate the changes made to address the comments made by previous reviewers.
## Important Dates


<table class="table table-striped">
    <tbody>
        <tr>
          <td>Paper Submission Deadline</td>
          <td>July 25, 2025 (12:00 Pacific time)</td>
        </tr>
        <tr>
          <td>Notification to Authors</td>
          <td>Aug 8, 2025</td>
        </tr>
        <tr>
          <td>Camera-Ready Deadline</td>
          <td>Aug 11, 2025 (12:00 Pacific time)</td>
        </tr>
    </tbody>
</table>

#### Registration
Workshop registration will be handled by the ACM-MM-2025 main conference committee. Please follow the ACM-MM-2025 [website](https://acmmm2025.org/registration/) for related information.

#### Presentation Instructions
Please prepare a 10-minute presentation for your accepted paper (7-minute presentation and 3-min Q&A).

In every conference session, the organizers will provide a MacBook laptop with adapters, USB sticks (USB C & 3.2) and laser pointers, running Keynote, Preview and Microsoft Powerpoint. Please use this laptop; do not use your own. It is essential, for the smooth running of each session, that all speakers upload their presentations to the podium laptop BEFORE the session begins, so the organizers ask all speakers to arrive at their session at least 15 minutes before the scheduled start-time to meet the session chair and upload the presentation to the podium laptop.

Please send a copy of your presentation to shreya.ghosh@uq.edu.au and parul@monash.edu. You will need to bring your PowerPoint presentation on a USB with you to the Conference. If you have any video files in your presentation, please have these files saved separately on your USB.

More details: [https://acmmm2025.org/information-for-presenters/](https://acmmm2025.org/information-for-presenters/) 

## Workshop Schedule

#### Monday, 27th Oct 2025
Time zone:	Dublin time [GMT +1 hour](https://www.timeanddate.com/time/zone/ireland/dublin)

Location: Radisson, Goldsmiths 3

Dublin Royal Convention Centre / Radisson

More details: [ACM MM full-program page](https://acmmm2025.org/acm-mm25-conference-agends/)

<table class="table table-striped">
    <tbody>
        <tr>
          <td> 13:30pm - 13:35pm </td>
          <td>Opening and welcome</td>
        </tr>
        <tr>
          <td>13:35pm - 14:15pm </td>
          <td>Keynote "Designing Computational Tools for Behavioral and Clinical Science" by Prof. Albert Ali Salah </td>
        </tr>
        <tr>
          <td>14:15pm - 14:25pm </td>
          <td>Paper 1: 	VLM-Guided Toddler Behavior Recognition from Semi-Structured Triadic Play Videos </td>
        </tr> 
      <tr>
          <td>14:25pm - 14:35pm</td>
          <td>Paper 2: 	Atoms of Thought: Universal EEG Representation Learning with Microstates </td>
        </tr>
      <tr>
        <tr>
          <td>14:35pm - 14:45pm</td>
          <td>Paper 3: 	EmoSync: Multi-Stage Reasoning with Multimodal Large Language Models for Fine-Grained Emotion Recognition </td>
        </tr>
      <tr>
          <td>14:45pm - 14:55pm</td>
          <td>Paper 4: 	MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition from Physiological Signals </td>
        </tr>
      <tr>
          <td>14:45pm - 14:55pm</td>
          <td>Paper 5: Zero-shot Emotion Annotation in Facial Images Using Large Multimodal Models: Benchmarking and Prospects for Multi-Class, Multi-Frame Approaches	</td>
        </tr>
      <tr>
          <td>14:55pm - 15:05pm</td>
          <td>Paper 6: Personality-Aware Engagement Prediction in Online Learning	</td>
        </tr>
      <tr>
      <tr>
        <td>15:05pm - 15:30pm</td>
          <td>Break (Afternoon Tea)</td>
        </tr>
        <tr>
          <td>15:30pm - 15:40pm</td>
          <td>Paper 7: SketchDancing: A Text-Driven Framework for Vector Sketch Animation Generation </td>
        </tr>
      <tr>
          <td>15:40pm - 15:50pm</td>
          <td>Paper 8: High-Fidelity Temporal Modeling of Facial Expression: AWavelet and LSTM Approach on Action Units Sequences	</td>
        </tr>
            <tr>
          <td>15:50pm - 16:00pm</td>
          <td>Paper 9: Personalized Animations for Affective Feedback: Generative AI Helps to Visualize Skin Conductance </td>
        </tr>
            <tr>
          <td>16:00pm - 16:10pm</td>
          <td>Paper 10: D4-Net: Detecting Deepfakes using a Dual-branch Deep learner	</td>
        </tr>
            <tr>
          <td>16:10pm - 16:20pm</td>
          <td>Paper 11: Face the Sound: Synthesizing Listener Facial Motion from Speaker Speech	</td>
        </tr>
      <tr>
        <tr>
          <td>16:20pm - 16:30pm</td>
          <td>Closing Remarks</td>
        </tr>
    </tbody>
</table>


## Invited Keynote Speaker
<div class="row">
    <div class="col-3">
        <a href="https://webspace.science.uu.nl/~salah006/">
            <img class="speaker-pic pull-right" src="/2024/je.jpeg" />
        </a>
    </div>
    <div class="col">
        <a href="https://webspace.science.uu.nl/~salah006/"><span class="fs-3">Prof. Albert Ali Salah <span class="fs-3"></span></a>
        <h6 class="fs-5">Utrecht University </h6>
      <Strong>Biography:</Strong> Prof. Salah is a professor and chair of Social and Affective Computing at the Information and Computing Sciences Department of Utrecht Univ. He obtained his PhD in 2007 from Bogazici University, and worked at CWI, University of Amsterdam, Nagoya University, and Bogazici University, before initiating the Social and Affective Computing group at Utrecht. His research is broad, but mainly uses pattern recognition and machine learning for computer analysis of human behavior. In his group, members work on individual behaviors (such as facial expression analysis), dyadic and group behaviours (e.g. child-parent or patient-doctor interactions), and on computational social science (e.g. mobile phone based analysis of migration and mobility). He was the coordinator of the Data for Refugees (D4R) Challenge between 2016-2019. He currently serve in the Steering Boards of ACM ICMI and IEEE FG, as an associate editor of journals Pattern Recognition and Int. Journal on Human-Computer Studies, and as VP Conferences for IEEE Biometrics Council.
    </div>
</div>
<br>

## Organizers

<div class="container p-0">
  <div class="row">
    <div class="col">
        <a href="https://eecs.uq.edu.au/profile/12367/shreya-ghosh">
            <img class="organizer-pic" src="/2023/img/people/ShreyaGhosh.jpg"/> 
        </a>
        <div class="people-name orgnizer-people-name">
            <a href="https://eecs.uq.edu.au/profile/12367/shreya-ghosh">Shreya Ghosh</a>
            <h6 class="uni-name">The University of Queensland</h6>
        </div>
    </div>
    <div class="col">
        <a href="https://research.monash.edu/en/persons/zhixi-cai">
            <img class="organizer-pic" src="/2023/img/people/zc.jpeg">
        </a>
        <div class="people-name orgnizer-people-name">
            <a href="https://research.monash.edu/en/persons/zhixi-cai">Zhixi Cai</a>
            <h6 class="uni-name">Monash University</h6>
        </div>
    </div>
    <div class="col">
        <a href="https://www.flinders.edu.au/people/abhinav.dhall">
            <img class="organizer-pic" src="/2023/img/people/AbhinavDhall.jpg">
        </a>
        <div class="people-name orgnizer-people-name">
            <a href="https://research.monash.edu/en/persons/abhinav-dhall">Abhinav Dhall</a>
            <h6 class="uni-name">Monash University</h6>
        </div>
    </div>    
    <div class="col">
        <a href="https://researchprofiles.canberra.edu.au/en/persons/roland-goecke">
            <img class="organizer-pic" src="/2023/img/people/RolandGoetcke.jpg">
        </a>
        <div class="people-name orgnizer-people-name">
            <a href="https://researchprofiles.canberra.edu.au/en/persons/roland-goecke">Roland Goecke</a>
            <h6 class="uni-name">UNSW Canberra</h6>
        </div>
    </div>
    <div class="col">
        <a href="https://staffportal.curtin.edu.au/staff/profile/view/tom-gedeon-5e48a1fd/">
            <img class="organizer-pic" src="/2023/img/people/TomGedeon.jpg">
        </a>
        <div class="people-name orgnizer-people-name">
            <a href="https://staffportal.curtin.edu.au/staff/profile/view/tom-gedeon-5e48a1fd/">Tom Gedeon</a>
            <h6 class="uni-name">Curtin University</h6>
        </div>
    </div>
  </div>
</div>

<style>
.organizer-pic {
    width: 150px;
    height: 150px;
}
.uni-name {
    max-width: 150px
}

.people-name {
    max-width: 150px;
} 

.orgnizer-people-name {
    text-align: center;
}

.speaker-pic, .organizer-pic {
    border-radius: 50%;
}

.banner-pic {
    width: 900px;
    height: auto;
}
</style>

#### Contact
Please contact us if you have any questions.
<br>
Email: shreya.ghosh@uq.edu.au, Zhixi.Cai@monash.edu, abhinav.dhall@monash.edu

Image Source: Wall-E 

CMT ACKNOWLEDGMENT: The Microsoft CMT service was used for managing the peer-reviewing process for this conference. This service was provided for free by Microsoft and they bore all expenses, including costs for Azure cloud services as well as for software development and support.



